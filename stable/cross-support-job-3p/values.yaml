# Default values for cross-support-job
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image: onedata/rest-cli:18.02.1
imagePullPolicy: IfNotPresent

wait_for:
  image: onedata/k8s-wait-for:v1.2
  imagePullPolicy: IfNotPresent

# Specifies if users job should wait for release to start
wait_for_release:
  enabled: true

# Specifies if groups job should wait for users job to end
wait_for_users:
  enabled: true

# Specifies if groups-join job should wait for groups job to end
wait_for_groups:
  enabled: true

# Specifies if spaces job should wait for groups-join job to end
wait_for_groups_join:
  enabled: true

# Specifies if supports job should wait for luma service to start
wait_for_luma:
  enabled: true

# Specifies if luma job should wait for spaces job to end
wait_for_spaces:
  enabled: true

# You can enable automatic deployment od the environment
# that this chart initializes
onedata-3p:
  enabled: true

# Or manually specify a helm release name of the already
# deployed environment
# releaseName: dev

# Run luma configuration job
lumaJobEnabled: false

# Set to {} to disable group creation job
groups: {}

# Set to {} to disable user creation job
users:
  - &user_joe
    name: joe
    firstName: Joe
    lastName: Morgan
    idps:
      onepanel:
        enabled: true
        type: regular
        mode: config
    password: password
    email: joe@example.com
    oneclient: [ krakow ]

# Set to {} to disable space creation
# and support configuration jobs
spaces:
  - name: "krk-3"
    user: *user_joe 
    supports:
      - provider: "krakow"
        storage_name: "ceph"
        size: '1000000000'
  - name: "par-n"
    user: *user_joe 
    supports:
      - provider: "paris"
        storage_name: "nfs"
        size: '1000000000'
  - name: "lis-3"
    user: *user_joe 
    supports:
      - provider: "lisbon"
        storage_name: "s3"
        size: '1000000000'
  - name: "krk-n-par-3"
    user: *user_joe 
    supports:
      - provider: "krakow"
        storage_name: "nfs"
        size: '1000000000'
      - provider: "paris"
        storage_name: "s3"
        size: '1000000000'
  - name: "krk-3-lis-c"
    user: *user_joe 
    supports:
      - provider: "krakow"
        storage_name: "s3"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "ceph"
        size: '1000000000'
  - name: "par-n-lis-c"
    user: *user_joe 
    supports:
      - provider: "paris"
        storage_name: "nfs"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "ceph"
        size: '1000000000'
  - name: "krk-3-par-c-lis-n"
    user: *user_joe 
    supports:
      - provider: "krakow"
        storage_name: "s3"
        size: '1000000000'
      - provider: "paris"
        storage_name: "ceph"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "nfs"
        size: '1000000000'
  - name: "krk-3-par-c-lis-n"
    user: *user_joe 
    supports:
      - provider: "krakow"
        storage_name: "s3"
        size: '1000000000'
      - provider: "paris"
        storage_name: "ceph"
        size: '1000000000'
      - provider: "lisbon"
        storage_name: "nfs"
        size: '1000000000'  
  - name: "krk-g"
    user: *user_joe 
    supports:
      - provider: "krakow"
        storage_name: "gluster"
        size: '1000000000' 
  - name: "krk-g-par-3"
    user: *user_joe 
    supports:
      - provider: "krakow"
        storage_name: "gluster"
        size: '1000000000'
      - provider: "paris"
        storage_name: "ceph"
        size: '1000000000'

# The generalization of nodeSelector.
# Allows for more fine grained controls over which
# nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# List of taints which are tolerated by the pods 
# when nodes are selected by a kubernetes scheduler
# https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: {}

# Specify a map of key-value pairs. For the pod 
# to be eligible to run on a node, the node 
# must have each of the indicated key-value pairs as labels
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}