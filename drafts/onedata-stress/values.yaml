# Default values for onedata-stress
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# How many concurrent jobs should be created
job_replicas: 2

# If the job fails how many times should it be retried
backoffLimit: 1

# wait-for-* containers parameters
wait_for:
  image: onedata/k8s-wait-for:v1.1
  imagePullPolicy: IfNotPresent

# getClientToken container parameters
getClientToken:
  # False if you want to disbale token prereching
  enabled: false
  
  image: onedata/rest-cli:18.02.0-rc4
  imagePullPolicy: IfNotPresent

  # Onezone host
  onezone_host: develop-onezone.develop.svc.dev.onedata.uk.to
  # Onezone REST api credentials
  basicAuth: admin:password


# Compute container
compute:
  image: severalnines/sysbench:latest

  # The computation can have 3 parts: preparation, computation (execution) and cleaning.
  # Arguments for each part can be set here.
  prepare_args: --test=fileio --file-num=1 --file-block-size=1M --file-total-size=100M --file-test-mode=seqrd --file-io-mode=sync prepare
  compute_args: --test=fileio --file-num=1 --file-block-size=1M --file-total-size=100M --file-test-mode=seqrd --file-io-mode=sync run
  clean_args: ""

  # Space name
  spaceName: "krk-3"

  # Chose the folder where spaces are mounted
  dataMountPoint: /mnt/oneclient

  # As the entrypoint of this container will be frequently  modified
  # it's code is placed here.
  # The framework of the test requirers to just edit the bodies of functions: prepare, compute, clean.
  # The example below uses sysbench to perform simple preperation and then tests.
  command:
    - "sh"
    - "-c"
    - >
      work_dir="$dataMountPoint/$spaceName" ;
      prepare() {
        echo "Changing working directo  ry to: $work_dir" ;
        cd "$work_dir" ;
        pwd ;
        printf "$(date) " ; echo "Starting sysbench prep." ;
        sysbench $prepare_args ;
        printf "$(date) " ; echo "sysbench prep end" ;
      } ;
      compute() {
        printf "$(date)" ; echo "Starting computation sysbench run." ;
        sysbench $compute_args ;
        printf "$(date) " ; echo "sysbench compute end" ;
      } ;
      clean() {
        printf "$(date)" ; echo "Starting cleaning." ;
        echo "No cleaning needed." ;
        printf "$(date) " ; echo "clean end" ;
      } ;
      echo "Waiting for oneclient..." ;
      while [ ! -f /monitor/oneclient-started ] ; do sleep 1 ; done ;
      echo "Oneclient ready." ;
      echo "Reached compute point." ;
      echo "Waiting for all other jobs to reach this point before running sysbench prepare." ;
      echo 1 > /monitor/wait-for-compute-prep-barrier ;
      while [ ! -f /monitor/wait-for-all-jobs-prep-barrier ] ; do sleep 1 ; done ;
      prepare ;
      echo "Waiting for all other jobs to reach this point before running sysbench run." ;
      echo 1 > /monitor/wait-for-compute-run-barrier ;
      while [ ! -f /monitor/wait-for-all-jobs-run-barrier ] ; do sleep 1 ; done ;
      compute ;
      echo "Waiting for all other jobs to reach this point before running cleaning procedure." ;
      echo 1 > /monitor/wait-for-compute-clean-barrier ;
      while [ ! -f /monitor/wait-for-all-jobs-clean-barrier ] ; do sleep 1 ; done ;
      clean ;
      printf "$(date)" ; echo "The compute container finished." ;
      printf "$(date)" ; echo "Signaling oneclient to finish." ;
      echo 1 > /monitor/oneclient-can-exit ;

# the job synchronizaton container that provides barrier functionality
# for the compute container using redis database
syncBarier:
  image: bash:4

   # Redis server address, default value: {{ .Release.Name }}-redis
  redisHost: ""

   # Redis server port
  redisPort: 6379

# Grafana prefix used to report data, the prefix example:
# template:
#   exper.${experiment}.date.${date}.args.${args}.release.${release}.dc.${dc}.cluster.${cluster}.host.${host}.k8s_pod.${k8s_pod}.comp.${computation}"
# with data filled in:
#   exper.orzech-test.date.2018-02-21_11:41:14.args.monitoringtype_graphite_monitoringlevelfull_monitoringperiod_10_graphiteurl_tcp:gocarbon,mon,svc,dev,onedata,uk,to_graphitenamespaceprefix__forcedirectio_mntoneclient.release.release18020-beta1.dc.cyf.cluster.k8s.host.k8s-cyfronet-1-node-4.k8s_pod.orzech-j-oneclient-jobs-nqv26
grafanaPrefix:
  experiment: orzech-test
  # date: # computed by helm in the format 2018-02-21_11:41:14
  # args computed in the pod's entry point
  release: "release18020-beta1"
  dc: cyf
  cluster: k8s
  # host from fieldPath
  # k8s_pod from fieldPath
  # computation: naive_bench

# Oneclient container parameters
oneclient:
  image: docker.onedata.org/oneclient:ID-0edc4939a0

  # Graphite prefix template that gets values from container env
  graphite_prefix: exper.${experiment}.date.${date}.args.${args}.release.${release}.dc.${dc}.cluster.${cluster}.host.${MY_NODE_NAME}.k8s_pod.${MY_POD_NAMESPACE}-${MY_POD_NAME}
  
  # Oneclient arguments
  args: --monitoring-type graphite --monitoring-level-full --monitoring-period 10 --graphite-url "$GRAFANA_URL" --graphite-namespace-prefix "$graphite_prefix" --force-direct-io ${MOUNT_POINT}

  # Grafana url used by oneclient to report data
  grafanaUrl: go-carbon.mon.svc.dev.onedata.uk.to

  # Onedata access token
  accessToken: "MDAxNWxvY2F00aW9uIG9uZXpvbmUKMDAzMGlkZW500aWZpZXIgYWRhMmJlNzUxNDUxMjJlZmRkMzU2NjgwZGExNzJkMTgKMDAxYWNpZCB00aW1lIDwgMTU1MDYwOTI2MQowMDJmc2lnbmF00dXJlIEH49m3Oj4ZIRaPTRRSCI01QcO575Lqt8MswQhL1CVmdPCg"

  # Oneclient will connect to this provider.
  provider_host: release18020-beta1-oneprovider-krakow.release18020-beta1.svc.dev.onedata.uk.to

  # Where spaces should be mounted
  mountPoint: /mnt/oneclient

  # Should oneclient-oneprovider connection allow for insecured https
  insecure: "true"


# Redis database config
# Full documentation can be found at:
# https://github.com/kubernetes/charts/blob/master/stable/redis/values.yaml
redis:
  image: bitnami/redis:4.0.7-r0
  imagePullPolicy: IfNotPresent
  serviceType: ClusterIP
  securityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  usePassword: false
  args:
  persistence:
    enabled: false
  metrics:
    enabled: false
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
  nodeSelector: {}
  tolerations: []
  podLabels: {}
  podAnnotations: {}
  networkPolicy:
    enabled: false
    allowExternal: true
  service:
    annotations: {}
    loadBalancerIP: